services:

  llm: &llm
    image: ollama/ollama:latest
    profiles: ["linux"]
    networks:
      - net

  llm-gpu:
    <<: *llm
    profiles: ["linux-gpu"]
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  pull-model:
    image: genai-stack/pull-model:latest
    build:
      context: .
      dockerfile: pull_model.Dockerfile
    environment:
      - OLLAMA_BASE_URL=${OLLAMA_BASE_URL-http://host.docker.internal:11434}
      - LLM=${LLM-llama2}
    networks:
      - net
    tty: true

  database:
    # user: neo4j:neo4j
    image: neo4j:5.11
    ports:
      - 7687:7687
      - 7474:7474
    volumes:
      - $PWD/data:/data
    environment:
      - NEO4J_AUTH=${NEO4J_USERNAME-neo4j}/${NEO4J_PASSWORD-password}
      - NEO4J_PLUGINS=["apoc"]
      - NEO4J_db_tx__log_rotation_retention__policy=false
      - NEO4J_dbms_security_procedures_unrestricted=apoc.*
    healthcheck:
        test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider localhost:7474 || exit 1"]
        interval: 15s
        timeout: 30s
        retries: 10
    networks:
      - net


  rag_bot:
    build:
      context: .
      dockerfile: rag_bot.Dockerfile
    environment:
      - NEO4J_URI=${NEO4J_URI-neo4j://database:7687}
      - NEO4J_PASSWORD=${NEO4J_PASSWORD-password}
      - NEO4J_USERNAME=${NEO4J_USERNAME-neo4j}
      - OLLAMA_BASE_URL=${OLLAMA_BASE_URL-http://host.docker.internal:11434}
      - LLM=${LLM-llama2}
      - EMBEDDING_MODEL=${EMBEDDING_MODEL-sentence_transformer}
    networks:
      - net
    depends_on:
      database:
        condition: service_healthy
      pull-model:
        condition: service_completed_successfully
    ports:
      - 8503:8503

  
  # kg_extractor:
  #   build:
  #     context: .
  #     dockerfile: kg_extractor.Dockerfile
  #   environment:
  #     - NEO4J_URI=${NEO4J_URI-neo4j://database:7687}
  #     - NEO4J_PASSWORD=${NEO4J_PASSWORD-password}
  #     - NEO4J_USERNAME=${NEO4J_USERNAME-neo4j}
  #     - OLLAMA_BASE_URL=${OLLAMA_BASE_URL-http://host.docker.internal:11434}
  #     - LLM=${LLM-llama2}
  #     - EMBEDDING_MODEL=${EMBEDDING_MODEL-sentence_transformer}
  #   networks:
  #     - net
  #   depends_on:
  #     database:
  #       condition: service_healthy
  #     pull-model:
  #       condition: service_completed_successfully
  #   ports:
  #     - 8502:8502

networks:
  net:
